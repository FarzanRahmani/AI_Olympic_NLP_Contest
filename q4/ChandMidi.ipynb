{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeb2e57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df8a64c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ca16549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    df = df.dropna()\n",
    "    # df['Review'] = df['Review'].fillna('').astype(str)\n",
    "    df['Review'] = df['Review'].str.replace('[^\\w\\s]', '')\n",
    "    df['Review'] = df['Review'].str.lower()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8490568",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.csv')\n",
    "train_data = clean_data(train_data)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afe17eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test.csv')\n",
    "test_data = clean_data(test_data)\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "214a161b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train data shape:\", train_data.shape)\n",
    "print(\"Test data shape:\", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca6c9f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some preprocessing \n",
    "\n",
    "class IMDbDataset(Dataset):\n",
    "    def __init__(self, reviews, ratings, tokenizer, max_length):\n",
    "        self.reviews = reviews\n",
    "        self.ratings = ratings\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        review = str(self.reviews[idx])\n",
    "        rating = self.ratings[idx] if self.ratings is not None else 0\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            review,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'review_text': review,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'rating': torch.tensor(rating, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
    "MAX_LENGTH = 256\n",
    "\n",
    "train_reviews, val_reviews, train_ratings, val_ratings = train_test_split(\n",
    "    train_data['Review'], train_data['Rating'], test_size=0.1, random_state=42)\n",
    "\n",
    "train_dataset = IMDbDataset(train_reviews.values, train_ratings.values, tokenizer, MAX_LENGTH)\n",
    "val_dataset = IMDbDataset(val_reviews.values, val_ratings.values, tokenizer, MAX_LENGTH)\n",
    "test_dataset = IMDbDataset(test_data['Review'].values, None, tokenizer, MAX_LENGTH)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cef0afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modeling \n",
    "class IMDbRegressionModel(torch.nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(IMDbRegressionModel, self).__init__()\n",
    "        self.xlm_roberta = XLMRobertaForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.xlm_roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return outputs.logits\n",
    "\n",
    "model = IMDbRegressionModel('xlm-roberta-base')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "def train_epoch(model, data_loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    # for batch in data_loader:\n",
    "    for batch in tqdm(data_loader, total=len(data_loader)):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        ratings = batch['rating'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = torch.nn.MSELoss()(outputs.squeeze(), ratings)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actual = []\n",
    "    with torch.no_grad():\n",
    "        # for batch in data_loader:\n",
    "        for batch in tqdm(data_loader, total=len(data_loader)):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            ratings = batch['rating'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            predictions.extend(outputs.squeeze().tolist())\n",
    "            actual.extend(ratings.tolist())\n",
    "    \n",
    "    return r2_score(actual, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a298463c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 4\n",
    "best_r2 = -float('inf')\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "    val_r2 = evaluate(model, val_loader, device)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val R2: {val_r2:.4f}\")\n",
    "    \n",
    "    if val_r2 > best_r2:\n",
    "        best_r2 = val_r2\n",
    "        best_model = model.state_dict()\n",
    "\n",
    "model.load_state_dict(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a76737d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d30b9441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict test samples\n",
    "# submission = pd.DataFrame()\n",
    "# submission\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    # for batch in test_loader:\n",
    "    for batch in tqdm(test_loader, total=len(test_loader)):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        predictions.extend(outputs.squeeze().tolist())\n",
    "\n",
    "submission = pd.DataFrame({'Rating': predictions})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Submission file created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c2ee91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "if not os.path.exists(os.path.join(os.getcwd(), 'ChandMidi.ipynb')):\n",
    "    %notebook -e ChandMidi.ipynb\n",
    "\n",
    "def compress(file_names):\n",
    "    print(\"File Paths:\")\n",
    "    print(file_names)\n",
    "    compression = zipfile.ZIP_DEFLATED\n",
    "    with zipfile.ZipFile(\"result.zip\", mode=\"w\") as zf:\n",
    "        for file_name in file_names:\n",
    "            zf.write('./' + file_name, file_name, compress_type=compression)\n",
    "            \n",
    "submission.to_csv('submission.csv', index=False)\n",
    "file_names = ['ChandMidi.ipynb', 'submission.csv']\n",
    "compress(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "471250df",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "    val_r2 = evaluate(model, val_loader, device)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val R2: {val_r2:.4f}\")\n",
    "    \n",
    "    if val_r2 > best_r2:\n",
    "        best_r2 = val_r2\n",
    "        best_model = model.state_dict()\n",
    "\n",
    "model.load_state_dict(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2911192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict test samples\n",
    "# submission = pd.DataFrame()\n",
    "# submission\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    # for batch in test_loader:\n",
    "    for batch in tqdm(test_loader, total=len(test_loader)):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        predictions.extend(outputs.squeeze().tolist())\n",
    "\n",
    "submission = pd.DataFrame({'Rating': predictions})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Submission file created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22944769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "if not os.path.exists(os.path.join(os.getcwd(), 'ChandMidi.ipynb')):\n",
    "    %notebook -e ChandMidi.ipynb\n",
    "\n",
    "def compress(file_names):\n",
    "    print(\"File Paths:\")\n",
    "    print(file_names)\n",
    "    compression = zipfile.ZIP_DEFLATED\n",
    "    with zipfile.ZipFile(\"result.zip\", mode=\"w\") as zf:\n",
    "        for file_name in file_names:\n",
    "            zf.write('./' + file_name, file_name, compress_type=compression)\n",
    "            \n",
    "submission.to_csv('submission.csv', index=False)\n",
    "file_names = ['ChandMidi.ipynb', 'submission.csv']\n",
    "compress(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09a255eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "if not os.path.exists(os.path.join(os.getcwd(), 'ChandMidi.ipynb')):\n",
    "    %notebook -e ChandMidi.ipynb\n",
    "\n",
    "def compress(file_names):\n",
    "    print(\"File Paths:\")\n",
    "    print(file_names)\n",
    "    compression = zipfile.ZIP_DEFLATED\n",
    "    with zipfile.ZipFile(\"result.zip\", mode=\"w\") as zf:\n",
    "        for file_name in file_names:\n",
    "            zf.write('./' + file_name, file_name, compress_type=compression)\n",
    "            \n",
    "submission.to_csv('submission.csv', index=False)\n",
    "file_names = ['ChandMidi.ipynb', 'submission.csv']\n",
    "compress(file_names)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
